---
title: "Coursera Data Science Capstone Milestone Report"
author: "Lisa Rodgers"
date: "2017-02-16"
output: html_document
---
  
#Introduction
This is the milestone report for **Johns Hopkins Data Science Specialization Capstone Project**. The purpose of this milestone report is to show an understanding about the project and current progress. It is an attempt to start further discussion and obtain constructive feedback from peers. Since the intended audience for this project is a non-data scientist manager, the code will be seperated from the report and kept to a minimum, while explaining how the data has been transformed.

Executive Summary
------------------------

The objective of this Capstone Project is to produce a predictive text algorithm written in R that based on a user's text input will suggest the next 8 most likely words to be entered.

As the user inputs characters the set of characters will be compared to text against a word list. The predicted word will be the word that has the highest probability following the previous word or multi-word phrase.

In the current project stage the dataset has been downloaded from Coursera and Swiftkey. Some initial exploratory data analysis has been performed along with some data preparation in order to proceed with the predictive modeling and construction of the end user application.

The next objective is to find the optimal sample size from the dataset required to build a corpus on which to train the prediction algorithm. 

The Problem
-----------------------

A user interface will need to be build to process the users input and predict the highest probability following the previous word or multi-word phrase. Problems that could arise are such as how to handle undesirable features within the dataset such as non-English words, twitter handles, email addresses, abbreviations and contractions, numbers and whitespace. 

One of the largest challenges present is how to achieve total coverage of all possible combinations. This algorithm will need to process large amounts of data quickly in order to keep the users attention. To achieve this a minimal
amount of data will be needed to maximize coverage.


Deliverables for this milestone
-------------------------------

The main deliverables are: 

* Demonstrate that you've downloaded the data and have successfully loaded it in.
* Create a basic report of summary statistics about the data sets.
* Report any interesting findings that you amassed so far.


Download Dataset
The training dataset is downloadable from [here](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip)

```{r "Downloading Data"}

setwd("L:\\Cousera\\Capstone\\")

url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
destFile <- "data/cs.zip"
output <- "L:\\Cousera\\Capstone\\data"

if (!file.exists(destFile)) {
  download.file(url, destFile)
  unzip(destfile,exdir=output) 
}


```

Data Summary
--------------------------------

The dataset includes 3 sources of textual data namely blogs, news and twitter. Each type has 4 languages including Russian, Finnish, German and English. The scope of this proect will only include the english version of all three types. 

Files used are as follows:
  
* en_US.blogs.txt
* en_US.twitter.txt
* en_US.news.txt

The following table summarizes the characteristics of the three datasets, including file size and number of lines.



```{r "Initializing Dataset", echo=FALSE, warning=FALSE,results='hide',include=FALSE}

# Libraries used
libs <- c("tm", "dplyr", "SnowballC", "stringi","knitr","ggplot2","gridExtra")

lapply(libs, require, character.only = TRUE)
dirPath <- paste0(output ,"\\final\\en_US\\")
fileNames <-  c("en_US.blogs.txt","en_US.news.txt", "en_US.twitter.txt")

dataSetName <- c("blogs","news","twitter")

```


```{r "Loading Datasets", echo=FALSE,results='hide',include=FALSE, cache=TRUE}


data <- lapply(paste0(dirPath, fileNames ), readLines)
names(data) <- dataSetName

```

Preprocessing
-------------------------------

In the interest of efficiency and time, a sample of 1% of the blog, news and twitter dataset has been combined to explore for this milestone report. After sampling, the data goes through a couple of transformations, including: 

* changing letters to lower case 
* remove emoticons
* removing whitespace 
* remove punctuations 
* remove numbers 
* remove stop words 
* remove word stems

After processing, the summary of the datasets are in the following table. These transformations are done by turning the vectors of data into class corpus and using the 'tm_map' function under 'tm' library.

```{r, echo=FALSE, warning=FALSE, results='hide', include=FALSE, cache=TRUE}


# Compute statistics and summary info for each data type
WPL <- sapply(data,function(x) summary(stri_count_words(x))[c('Min.','Mean','Max.')])

# Add meaningful Rownames
rownames(WPL) <- c('Min WPL','Mean WPL','Max WPL')

#Create Stats data frame
stats <- data.frame(
  FileName=c("en_US.blogs","en_US.news","en_US.twitter"),      
  t(rbind(
    sapply(data,stri_stats_general)[c('Lines','Chars'),],
    Words=sapply(data,stri_stats_latex)['Words',],
    WPL)
  ))


```


```{r "Display Stats", echo=FALSE, warning=FALSE, cache=TRUE}

kable(stats)

```




```{r "Sampling Dataset", echo=FALSE, warning=FALSE, results='hide', include=FALSE, cache=TRUE}

# Set random seed for reproducibility and sample the data
set.seed(1218)
sampleBlogs <- data$blogs[sample(1:length(data$blogs), 0.01*length(data$blogs), replace=FALSE)]
sampleNews <- data$news[sample(1:length(data$news), 0.01*length(data$news), replace=FALSE)]
sampleTwitter <- data$twitter[sample(1:length(data$twitter), 0.01*length(data$twitter), replace=FALSE)]


# Remove unconvention/funny characters for sampled Blogs/News/Twitter
removeEmoticons <- function (x) {
  x <- iconv(x, "UTF-8", "ASCII", sub="")
}

cs_Blogs <- removeEmoticons(sampleBlogs)
cs_News <- removeEmoticons(sampleNews)
cs_Twitter <- removeEmoticons(sampleTwitter)


# Clean sample of 1% of each text file
combine.Data <- c(cs_Blogs,cs_News,cs_Twitter)



```

```{r "Cleanup Unsed Variables",echo=FALSE, warning=FALSE, results='hide'}

# Remove temporary variables
rm(data, cs_Blogs, cs_News, cs_Twitter)


```

```{r "Build Corpus and Clean Data", cache=TRUE}

build_corpus <- function (x = sampleData) {
  c <- VCorpus(VectorSource(x)) # Create corpus dataset
  c <- tm_map(c, tolower) # all characters to lowercase
  c <- tm_map(c, removePunctuation) # Remove punctuation
  c <- tm_map(c, removeNumbers) # Remove numbers
  c <- tm_map(c, stripWhitespace) # Remove whitespace
  c <- tm_map(c, removeWords, stopwords("english")) # remove english stop words
  c <- tm_map(c, stemDocument) # Stem the document
  c <- tm_map(c, PlainTextDocument) # Create plain text format
}
corpusData <- build_corpus(combine.Data)

rm(combine.Data)

#Run garbage collection to reclaim memory
gc()
```

Exploratory Analysis
------------------------------------

The frequencies of the words from blogs, news and twitter datasets are summarised in the wordCloud and three bar charts, plotted below. Only top 10 most frequent words are shown for each of the dataset in the dataset. 

It is noted that **stop words** have been removed after **punctuation** was removed, this has cause words like dont and cant to still be in the dataset. An adjustment will be needed to remove stopwords than punctuation in moving forward.


```{r "Getting top terms", echo=FALSE, cache=TRUE}

library(RWeka)
getTermTable <- function(corpusData, ngrams = 1, lowfreq = 50) {
    #create term-document matrix tokenized on n-grams
    tokenizer <- function(x) { NGramTokenizer(x, Weka_control(min = ngrams, max = ngrams)) 
    
    }
    
    tdm <- TermDocumentMatrix(corpusData, control = list(tokenize = tokenizer))
  
  #find the top term grams with a minimum of occurrence in the corpus
  top_terms <- findFreqTerms(tdm,lowfreq)
  top_terms_freq <- rowSums(as.matrix(tdm[top_terms,]))
  top_terms_freq <- data.frame(word = names(top_terms_freq), frequency = top_terms_freq)
  top_terms_freq <- arrange(top_terms_freq, desc(frequency))
  
  
}

top.terms <- list(3)
for (i in 1:3) {
 top.terms[[i]] <- getTermTable(corpusData, ngrams = i, lowfreq = 10)
}



```


```{r "Building Charts", echo=FALSE, warning=FALSE, cache=TRUE, fig.width=10, fig.height=4}

library(wordcloud)

# Set Plotting in 1 row 3 columns
par(mfrow=c(1, 3))
par(mar=c(0.5, 0.5, 0.5, 0.5))
pallets <- c("BuGn","OrRd","PuBu")

#wcLabel <- c("Unigram", "Bigrams", "Trigrams")
#wcFreq <- list(4000,4000,4000)

#wcTitle <- cbind(wcLabel,wcFreq)
#n <-names(top.terms[[1]])
#colnames(wcTitle) <- n
#wcTitle
#head(top.terms[[1]])
#top.terms[[1]] <-wcTitle[1,]
#top.terms[[1]] <-sort(top.terms[[1]]$frequency, decreasing = FALSE)
#top.terms[[2]] <- rbind(top.terms[[2]],wcTitle[2,])
#top.terms[[3]] <- rbind(top.terms[[3]],wcTitle[3,])


for (i in 1:3) {

 wordcloud(top.terms[[i]]$word, top.terms[[i]]$frequency,min.freq=1,  scale = c(3,0.5), max.words=200, random.order=FALSE, rot.per=0.45, fixed.asp = TRUE, use.r.layout = FALSE, colors=brewer.pal(9,pallets[i])
           
)

 
}
```

```{r "Displaying Graphs", echo=FALSE, warning=FALSE, results='hide', fig.width=10, fig.height=12}

plot.Graphs <- function (x = top.terms, N=10) {
  
  g1 <- ggplot(data = head(x[[1]],N), aes(x = reorder(word, - frequency), y = frequency)) + 
          geom_bar(stat = "identity", fill = "#009E73", colour = "black") + 
          ggtitle(paste("Top 10 Unigrams")) + 
          xlab("Unigrams") + ylab("Frequency") + 
          coord_flip() +
          theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  g2 <- ggplot(data = head(x[[2]],N), aes(x = reorder(word, -frequency), y = frequency)) + 
        geom_bar(stat = "identity", fill = "#E69F00", colour = "black") + 
        ggtitle(paste("Top 10 Bigrams")) + 
        xlab("Bigrams") + ylab("Frequency") + 
        coord_flip() +
        theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  g3 <- ggplot(data = head(x[[3]],N), aes(x = reorder(word, -frequency), y = frequency)) + 
        geom_bar(stat = "identity", fill = "#56B4E9",colour = "black") + 
        ggtitle(paste("Top 10 Trigrams")) + 
        xlab("Trigrams") + ylab("Frequency") + 
        coord_flip() +
        theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  library(gridExtra)
  # Put three plots into 1 row 3 columns
  gridExtra::grid.arrange(g1, g2, g3, ncol = 1)
}
 
plot.Graphs(x = top.terms, N = 10)

 
  
   

```


The Plan
-------------------------

The data has been cleaned and stacked for further work that will include exploring the frequencies of multiple-word phrases using n-gram analysis. A predictive model with multiple n-gram will develop a text predictive app for the final submission. The prediction will also list out a couple of most likely **next word** which are ranked by probabilities. The predictive app will be hosted as a Shiny app to predict the next word with up to 4-5 words input. 

### Final note
For readability purpose, this report hides most the R code that generated above data and plots.


Appendix
----------------------------

### Corpus Inspection

The text mining (TM) package will be used to help with the collection and analysis of the data. A Corpus, represents a collection of text documents. A corpus is an abstract concept, and there can exist several implementations in parallel. A VCorpus or Volatile Corpus are held fully in memory. This is denoted as volatile since once the R object is destroyed, the whole corpus is gone. 


```{R "Appendix", echo=FALSE, warning=FALSE}
inspect(corpusData[1:3])

```
### Stop Words
Stop words removed from corpus data set.

```{r "Displaying Stopword" , echo=FALSE, cache=TRUE }
stopwords()

```




